# -*- coding: utf-8 -*-
"""daily_stock_returns_and_decision_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJm3GoMhHXnRz9kM9RcXNQguElbcm38D

# LIBRARIES IMPORTATION
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from sklearn.neural_network import MLPClassifier,MLPRegressor
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import accuracy_score,precision_score,recall_score, f1_score,roc_curve,auc,r2_score,mean_squared_error,mean_absolute_error
import warnings
from sklearn.svm import SVC,SVR
from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor
warnings.filterwarnings("ignore")
import talib
import xgboost
import lightgbm
from sklearn.feature_selection import SelectKBest,f_classif
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense,Dropout,Reshape,Conv1D, MaxPooling1D,Flatten

"""# DATASET IMPORTATION AND MERGING"""

amx=pd.read_csv('American_express.csv')
fiv=pd.read_csv('Fiserv.csv')
vis=pd.read_csv('Visa.csv')
pay=pd.read_csv('Paypal.csv')
mst=pd.read_csv('Mastercard.csv')
amx['Company']='AMX'
fiv['Company']='FI'
vis['Company']='V'
pay['Company']='PYL'
mst['Company']='MA'
r=pd.read_csv('macro_data.csv')
# df=pd.concat([amx,fiv,vis,pay,mst],ignore_index=True)
# df=df.sort_values('Date')



amx_macro=pd.merge(amx,r,on='Date',how='inner')
fiv_macro=pd.merge(fiv,r,on='Date',how='inner')
vis_macro=pd.merge(vis,r,on='Date',how='inner')
pay_macro=pd.merge(pay,r,on='Date',how='inner')
mst_macro=pd.merge(mst,r,on='Date',how='inner')

amx_macro

s=amx_macro

"""# FEATURE ENGINEERING"""

def feature_engineer(data):
    data['rsi'] = talib.RSI(data['Close price'].values, timeperiod=14)
    data["7MA"]=talib.MA(data['Close price'],timeperiod=7)
    data["14MA"]=talib.MA(data['Close price'],timeperiod=14)
    data["21MA"]=talib.MA(data['Close price'],timeperiod=21)
    data["STD7"]=talib.STDDEV(data['Close price'],timeperiod=7)
    macd,sg,hist=talib.MACD(data['Close price'])
    data['MACD']=macd
    data['MACD_s']=sg
    data['Date']=pd.to_datetime(data['Date'])
    returns=data['Close price'].pct_change()
    data['returns']=returns
    data=data.dropna()
    threshold=0.01
    data['signal']=returns.apply(lambda x:1 if x>threshold else 0)
    data=data.dropna()
    return data

a=feature_engineer(mst_macro)
b=feature_engineer(amx_macro)
c=feature_engineer(pay_macro)
d=feature_engineer(fiv_macro)
e=feature_engineer(vis_macro)

a

b

c

d

e

df=pd.concat([a,b,c,d,e],ignore_index=True)
df=df.sort_values('Date')
# df.to_csv('merged_data.csv')

plt.figure(figsize=(15,7))
df.signal.value_counts().plot.bar()

"""# DATA EXPLORATION"""

df.info()

df.corr()



df.describe()

df.shape



amx=df[df['Company']=='AMX']
plt.figure(figsize=(18,9))
sns.lineplot(y='returns',x='Date',data=amx)
plt.title('Daily returns for American Express Company')

ma=df[df['Company']=='MA']
plt.figure(figsize=(18,9))
sns.lineplot(y='returns',x='Date',data=ma)
plt.title('Daily returns for MasterCard Company')

v=df[df['Company']=='V']
plt.figure(figsize=(18,9))
sns.lineplot(y='returns',x='Date',data=v)
plt.title('Daily returns for Visa Company')

fi=df[df['Company']=='FI']
plt.figure(figsize=(18,9))
sns.lineplot(y='returns',x='Date',data=fi)
plt.title('Daily returns for Fiserv Company')

pyl=df[df['Company']=='PYL']
plt.figure(figsize=(18,9))
sns.lineplot(y='returns',x='Date',data=pyl)
plt.title('Daily returns for Paypal Company')



"""# DATA SPLITTING"""

data=df

T='2021-01-01'
training=data[df['Date']<T]
testing=data[data['Date']>=T]

x=training.drop(['Company','Date','signal','returns'],axis=1)
y=training['returns']

x

select=SelectKBest(score_func=f_classif,k=10)
z=select.fit_transform(x,y)
flt=select.get_support()
s=x.columns
print(s[flt])

x=training[['Close price', 'D/E Ratio', 'ROIC', 'S&P index', '7MA', '14MA', '21MA','STD7', 'MACD', 'MACD_s']]
y=training[['returns']]

x_train=training[['Close price', 'D/E Ratio', 'ROIC', 'S&P index', '7MA', '14MA', '21MA','STD7', 'MACD', 'MACD_s']].values
y_train=training['returns'].values
x_test=testing[['Close price', 'D/E Ratio', 'ROIC', 'S&P index', '7MA', '14MA', '21MA','STD7', 'MACD', 'MACD_s']].values
y_test=testing['returns'].values

scaler = MinMaxScaler()
x_train= scaler.fit_transform(x_train)
x_test= scaler.transform(x_test)
scaler=StandardScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)

rf_1=RandomForestRegressor()
rf_1.fit(x_train,y_train)
y_pred_rf_1=rf_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_rf_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_rf_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_rf_1))
print('R-Squared Value:',rf_1.score(x_test,y_test))

xg_1=xgboost.XGBRegressor()
xg_1.fit(x_train,y_train)
y_pred_xg_1=xg_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_xg_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_xg_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_xg_1))
print('R-Squared Value:',xg_1.score(x_test,y_test))

lg_1=lightgbm.LGBMRegressor()
lg_1.fit(x_train,y_train)
y_pred_lg_1=lg_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_lg_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_lg_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_lg_1))
print('R-Squared Value:',lg_1.score(x_test,y_test))

dt_1=DecisionTreeRegressor()
dt_1.fit(x_train,y_train)
y_pred_dt_1=dt_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_dt_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_dt_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_dt_1))
print('R-Squared Value:',dt_1.score(x_test,y_test))

svm_1=SVR()
svm_1.fit(x_train,y_train)
y_pred_svm_1=svm_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_svm_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_svm_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_svm_1))
print('R-Squared Value:',svm_1.score(x_test,y_test))

lr_1=LinearRegression()
lr_1.fit(x_train,y_train)
y_pred_lr_1=lr_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_lr_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_lr_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_lr_1))
print('R-Squared Value:',(lr_1.score(x_test,y_test)))

mlp_1=MLPRegressor()
mlp_1.fit(x_train,y_train)
y_pred_mlp_1=mlp_1.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_mlp_1))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_mlp_1)))
print('MAE:',mean_absolute_error(y_test,y_pred_mlp_1))
print('R-Squared Value:',mlp_1.score(x_test,y_test))

cnn= Sequential()
cnn.add(Conv1D(64, 3, activation='relu', input_shape=(x_train.shape[1], 1)))
cnn.add(MaxPooling1D(2))
cnn.add(Flatten())
cnn.add(Dropout(0.2))
cnn.add(Dense(1))
cnn.compile(optimizer='adam', loss='mean_absolute_error')
cnn.fit(x_train, y_train, epochs=10)
cnn_pred = cnn.predict(x_test)
print('MAE',mean_absolute_error(y_test,cnn_pred))
print('MSE',mean_squared_error(y_test,cnn_pred))
print('RMSE',np.sqrt(mean_squared_error(y_test,cnn_pred)))
print('R-squared value',r2_score(y_test,cnn_pred))

# Build the LSTM model
lstm = Sequential()
lstm.add(LSTM(50, activation='relu', input_shape=(x_train.shape[1],1)))
lstm.add(Dense(1))
cnn.add(Dropout(0.2))
lstm.compile(optimizer='adam', loss='mse')
lstm.fit(x_train, y_train, epochs=10)
lstm_pred = lstm.predict(x_test)
print('MAE',mean_absolute_error(y_test,lstm_pred))
print('MSE',mean_squared_error(y_test,lstm_pred))
print('RMSE',np.sqrt(mean_squared_error(y_test,lstm_pred)))
print('R-squared value',r2_score(y_test,lstm_pred))



par_rf= {
    'n_estimators':[150,100,250,300,400],
    'min_samples_leaf':[1,2,4,6],
    'max_depth':[1,2,5,10],
    'min_samples_split':[2,4,6]

}

grid_rf= GridSearchCV(rf_1, par_rf, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_rf.fit(x_train,y_train)
best_rf= grid_rf.best_params_
print('Best parameters  for RF found:',best_rf)
rf=grid_rf.best_estimator_
rf.fit(x_train,y_train)
y_pred_rf=rf.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_rf))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_rf)))
print('MAE:',mean_absolute_error(y_test,y_pred_rf))
print('R-Squared Value:',r2_score(y_test,y_pred_rf))
print()
print()




par_xg= {
    'n_estimators':[150,100,300,400],
    'learning_rate':[0.1,0.001,1.0],
    'max_depth':[3,5,7],
    'subsample':[0.6,0.8,1.0]
}
grid_xg= GridSearchCV(xg_1, par_xg, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_xg.fit(x_train,y_train)
best_xg= grid_xg.best_params_
print('Best parameters  for Xgboost found:',best_xg)
xg=grid_xg.best_estimator_
xg.fit(x_train,y_train)
y_pred_xg=xg.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_xg))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_xg)))
print('MAE:',mean_absolute_error(y_test,y_pred_xg))
print('R-Squared Value:',r2_score(y_test,y_pred_xg))
print()
print()



par_lg= {
    'n_estimators':[100,300,400],
    'learning_rate':[0.1,0.2,0.4],
    'num_leaves':[5,10,15],
    'boosting_type':['dart','rf','gbdt']

}

grid_lg= GridSearchCV(lg_1, par_lg, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_lg.fit(x_train,y_train)
best_lg= grid_lg.best_params_
print('Best parameters  for lightgbm found:',best_lg)
lg=grid_lg.best_estimator_
lg.fit(x_train,y_train)
y_pred_lg=lg.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_lg))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_lg)))
print('MAE:',mean_absolute_error(y_test,y_pred_lg))
print('R-Squared Value:',r2_score(y_test,y_pred_lg))

par_lr= {
    'fit_intercept':[x for x in range(1,20)],
    'positive':[True, False],

}

grid_lr= GridSearchCV(lr_1, par_lr, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_lr.fit(x_train,y_train)
best_lr=grid_lr.best_params_
print('Best parameters  for LR found:', best_lr)
lr=grid_lr.best_estimator_
lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_lr))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_lr)))
print('MAE:',mean_absolute_error(y_test,y_pred_lr))
print('R-Squared Value:',r2_score(y_test,y_pred_lr))
print()
print()


par_svm= {
    'kernel':['linear','polynomial','radial'],
    'C':[0.1,1,10],
    'gamma':['auto','scale']
}

grid_svm= GridSearchCV(svm_1, par_svm, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_svm.fit(x_train,y_train)
best_svm=grid_svm.best_params_
print('Best parameters  for SVM found:', best_svm)
svm=grid_svm.best_estimator_
svm.fit(x_train,y_train)
y_pred_svm=svm.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_svm))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_svm)))
print('MAE:',mean_absolute_error(y_test,y_pred_svm))
print('R-Squared Value:',r2_score(y_test,y_pred_svm))
print()
print()



par_dt= {
    'max_depth':[5,10,20],
    'min_samples_split':[2,4,6],
    'max_features':[2,5,6],
    'min_samples_leaf':[2,4,6]
}

grid_dt= GridSearchCV(dt_1, par_dt, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_dt.fit(x_train,y_train)
best_dt=grid_dt.best_params_
print('Best parameters  for DT found:', best_dt)
dt=grid_dt.best_estimator_
dt.fit(x_train,y_train)
y_pred_dt=dt.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_dt))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_dt)))
print('MAE:',mean_absolute_error(y_test,y_pred_dt))
print('R-Squared Value:',r2_score(y_test,y_pred_dt))

par_ann= {
    'hidden_layer_sizes':[(100,),(50,50),(50,100,50)],
    'activation':['relu','tanh','logistic'],
    'solver':['lbfgs','sgd','adam'],
    'learning_rate':['adaptive','constant'],
    'alpha':[0.0001,0.001,0.1]

}

grid_ann= GridSearchCV(mlp_1, par_ann, n_jobs=-1, cv=3,scoring='neg_mean_squared_error')
grid_ann.fit(x_train,y_train)
best_ann=grid_ann.best_params_
print('Best parameters  for ANN found:', best_ann)
ann=grid_ann.best_estimator_
ann.fit(x_train,y_train)
y_pred_ann=ann.predict(x_test)
print('MSE:',mean_squared_error(y_test,y_pred_ann))
print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred_ann)))
print('MAE:',mean_absolute_error(y_test,y_pred_ann))
print('R-Squared Value:',r2_score(y_test,y_pred_ann))



T='2021-01-01'
training=df[df['Date']<T]
testing=df[df['Date']>=T]

x=training.drop(['Company','Date','signal'],axis=1)
y=training['signal']

"""# FEATURE SELECTION"""

select=SelectKBest(score_func=f_classif,k=10)
z=select.fit_transform(x,y)
flt=select.get_support()
s=x.columns
print(s[flt])

x=training[['Close price', 'S&P index', 'rsi', '7MA', '14MA', '21MA', 'STD7','MACD', 'MACD_s', 'returns']]
y=training[['signal']]



x_train=training[['Close price', 'S&P index', 'rsi', '7MA', '14MA', '21MA', 'STD7','MACD', 'MACD_s', 'returns']].values
y_train=training['signal'].values
x_test=testing[['Close price', 'S&P index', 'rsi', '7MA', '14MA', '21MA', 'STD7','MACD', 'MACD_s', 'returns']].values
y_test=testing['signal'].values

"""# NORMALIZING AND STANDARDIZING DATA"""

scaler = MinMaxScaler()
x_train= scaler.fit_transform(x_train)
x_test= scaler.transform(x_test)
scaler=StandardScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)



"""# ENSEMBLE ALGORITHMS"""

rf=RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred_rf=rf.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_rf))
print('Precision Score:',precision_score(y_test,y_pred_rf))
print('Recall Score:',recall_score(y_test,y_pred_rf))
print('F1-Score:',f1_score(y_test,y_pred_rf))

xg=xgboost.XGBClassifier()
xg.fit(x_train,y_train)
y_pred_xg=xg.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_xg))
print('Precision Score:',precision_score(y_test,y_pred_xg))
print('Recall Score:',recall_score(y_test,y_pred_xg))
print('F1-Score:',f1_score(y_test,y_pred_xg))

lg=lightgbm.LGBMClassifier()
lg.fit(x_train,y_train)
y_pred_lg=lg.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_lg))
print('Precision Score:',precision_score(y_test,y_pred_lg))
print('Recall Score:',recall_score(y_test,y_pred_lg))
print('F1-Score:',f1_score(y_test,y_pred_lg))



"""# TRADITIONAL ALGORITHMS"""

lr=LogisticRegression()
lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_lr))
print('Precision Score:',precision_score(y_test,y_pred_lr))
print('Recall Score:',recall_score(y_test,y_pred_lr))
print('F1-Score:',f1_score(y_test,y_pred_lr))

svm=SVC(probability=True)
svm=svm.fit(x_train,y_train)
y_pred_svm=svm.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_svm))
print('Precision Score:',precision_score(y_test,y_pred_svm))
print('Recall Score:',recall_score(y_test,y_pred_svm))
print('F1-Score:',f1_score(y_test,y_pred_svm))

dt=DecisionTreeClassifier()
dt.fit(x_train,y_train)
y_pred_dt=dt.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_dt))
print('Precision Score:',precision_score(y_test,y_pred_dt))
print('Recall Score:',recall_score(y_test,y_pred_dt))
print('F1-Score:',f1_score(y_test,y_pred_dt))

"""# NEURAL NETWORKS"""

ann=MLPClassifier()
ann.fit(x_train,y_train)
y_pred_ann=ann.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_ann))
print('Precision Score:',precision_score(y_test,y_pred_ann))
print('Recall Score:',recall_score(y_test,y_pred_ann))
print('F1-Score:',f1_score(y_test,y_pred_ann))

cnn= Sequential([
    Reshape(target_shape=(x_train.shape[1], 1), input_shape=(x_train.shape[1],)),
    Conv1D(62, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    cnn.add(Dropout(0.2)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])
cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
cnn.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))
y_pred_prob= cnn.predict(x_test)
y_pred_cnn = (y_pred_prob > 0.5).astype(float)
print('Accuracy Score:',accuracy_score(y_test,y_pred_cnn))
print('Precision Score:',precision_score(y_test,y_pred_cnn))
print('Recall Score:',recall_score(y_test,y_pred_cnn))
print('F1-Score:',f1_score(y_test,y_pred_cnn))

lstm = Sequential([
    Reshape(target_shape=(x_train.shape[1], 1), input_shape=(x_train.shape[1],)),
    LSTM(62, activation='relu'),
    Dropout(0.2),
    Dense(1,activation='sigmoid')
])
lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
lstm.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))
y_pred_prob= lstm.predict(x_test)
y_pred_lstm = (y_pred_prob > 0.5).astype(float)
print('Accuracy Score:',accuracy_score(y_test,y_pred_lstm))
print('Precision Score:',precision_score(y_test,y_pred_lstm))
print('Recall Score:',recall_score(y_test,y_pred_lstm))
print('F1-Score:',f1_score(y_test,y_pred_lstm))



# ROC Curve for Ensemble Algorithms
Q = rf.predict_proba(x_test)[:,1]
R = xg.predict_proba(x_test)[:,1]
S = lg.predict_proba(x_test)[:,1]
#ROC PLOT
fpr, tpr, _ = roc_curve(y_test, Q)
roc_auc = auc(fpr,tpr)
fpr1, tpr1, _ = roc_curve(y_test, R)
roc_auc1 = auc(fpr1,tpr1)
fpr2, tpr2, _ = roc_curve(y_test, S)
roc_auc2 = auc(fpr2,tpr2)
plt.figure(figsize=(10,7))

plt.plot(fpr, tpr, color='darkorange', label='Random Forest(AUC = %0.4f)' % roc_auc)
plt.plot(fpr1, tpr1, color='red', label='Xgboost (AUC = %0.4f)' % roc_auc1)
plt.plot(fpr2, tpr2, color='green', label='LightGbm (AUC = %0.4f)' % roc_auc2)
plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic curve For Ensemble Algorithms')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Traditional Algorithms
X = lg.predict_proba(x_test)[:,1]
Y = dt.predict_proba(x_test)[:,1]
Z = svm.predict_proba(x_test)[:,1]
#ROC PLOT
fpr, tpr, _ = roc_curve(y_test, X)
roc_auc = auc(fpr,tpr)
fpr1, tpr1, _ = roc_curve(y_test, Y)
roc_auc1 = auc(fpr1,tpr1)
fpr2, tpr2, _ = roc_curve(y_test, Z)
roc_auc2 = auc(fpr2,tpr2)
plt.figure(figsize=(10,7))

plt.plot(fpr, tpr, color='darkorange', label='Logistic Regression (AUC = %0.4f)' % roc_auc)
plt.plot(fpr1, tpr1, color='red', label='Decision Tree (AUC = %0.4f)' % roc_auc1)
plt.plot(fpr2, tpr2, color='green', label='SVM (AUC = %0.4f)' % roc_auc2)
plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic curve For Traditional Algorithms')
plt.legend(loc="lower right")
plt.show()

J= lstm.predict(x_test)[:,0]
Z = ann.predict_proba(x_test)[:,1]
R= cnn.predict(x_test)[:,0]
fpr, tpr, _ = roc_curve(y_test, J)
roc_auc = auc(fpr,tpr)
fpr1, tpr1, _ = roc_curve(y_test, R)
roc_auc1 = auc(fpr1,tpr1)
fpr2, tpr2, _ = roc_curve(y_test, Z)
roc_auc2 = auc(fpr2,tpr2)
plt.figure(figsize=(10,7))

plt.plot(fpr, tpr, color='darkorange', label='LSTM (AUC = %0.4f)' % roc_auc)
plt.plot(fpr1, tpr1, color='red', label='CNN (AUC = %0.4f)' % roc_auc1)
plt.plot(fpr2, tpr2, color='green', label='ANN (AUC = %0.4f)' % roc_auc2)
plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic curve For Neural Network Algorithms')
plt.legend(loc="lower right")
plt.show()

"""# Fine tuning Ensemble Algorithms"""

par_rf= {
    'n_estimators':[150,100,250,300,400],
    'min_samples_leaf':[1,2,4,6],
    'max_depth':[1,2,5,10],
    'min_samples_split':[2,4,6]

}

grid_rf= GridSearchCV(rf, par_rf, n_jobs=-1, cv=5,scoring='accuracy')
grid_rf.fit(x_train,y_train)
best_rf= grid_rf.best_params_
print('Best parameters  for RF found:',best_rf)
rf=grid_rf.best_estimator_
rf.fit(x_train,y_train)
y_pred_rf=rf.predict(x_test)
print('Accuracy Score Randm forest Model:',accuracy_score(y_test,y_pred_rf))
print('Precision Score Random forest Model:',precision_score(y_test,y_pred_rf))
print('Recall Score Random forest Model:',recall_score(y_test,y_pred_rf))
print('F1-Score Random forest Model:',f1_score(y_test,y_pred_rf))
print()
print()




par_xg= {
    'n_estimators':[150,100,300,400],
    'learning_rate':[0.1,0.001,1.0],
    'max_depth':[3,5,7],
    'subsample':[0.6,0.8,1.0]
}
grid_xg= GridSearchCV(xg, par_xg, n_jobs=-1, cv=5,scoring='accuracy')
grid_xg.fit(x_train,y_train)
best_xg= grid_xg.best_params_
print('Best parameters  for Xgboost found:',best_xg)
xg=grid_xg.best_estimator_
xg.fit(x_train,y_train)
y_pred_xg=xg.predict(x_test)
print('Accuracy Score Xgboost Model:',accuracy_score(y_test,y_pred_xg))
print('Precision Score Xgboost Model:',precision_score(y_test,y_pred_xg))
print('Recall Score Xgboost Model:',recall_score(y_test,y_pred_xg))
print('F1-Score Xgbost Model:',f1_score(y_test,y_pred_xg))
print()
print()



par_lg= {
    'n_estimators':[100,300,400],
    'learning_rate':[0.1,0.2,0.4],
    'num_leaves':[5,10,15],
    'boosting_type':['dart','rf','gbdt']

}

grid_lg= GridSearchCV(lg, par_lg, n_jobs=-1, cv=5,scoring='accuracy')
grid_lg.fit(x_train,y_train)
best_lg= grid_lg.best_params_
print('Best parameters  for lightgbm found:',best_lg)
lg=grid_lg.best_estimator_
lg.fit(x_train,y_train)
y_pred_lg=lg.predict(x_test)
print('Accuracy Score LightGBM Model:',accuracy_score(y_test,y_pred_lg))
print('Precision Score LightGBM Model:',precision_score(y_test,y_pred_lg))
print('Recall Score LightGBM Model:',recall_score(y_test,y_pred_lg))
print('F1-Score LightGBM Model:',f1_score(y_test,y_pred_lg))

"""# Fine tuning Traditional Algorithms"""

par_lr= {
    'fit_intercept':[x for x in range(1,20)],
    'solver':['saga','liblinear'],
    'penalty':['l1','l2'],
    'C':[0.001,0.01,0.1,1,10],

}

grid_lr= GridSearchCV(lr, par_lr, n_jobs=-1, cv=5,scoring='accuracy')
grid_lr.fit(x_train,y_train)
best_lr=grid_lr.best_params_
print('Best parameters  for LR found:', best_lr)
lr=grid_lr.best_estimator_
lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)
print('Accuracy Score Linear regression model:',accuracy_score(y_test,y_pred_lr))
print('Precision Score Linear regression model:',precision_score(y_test,y_pred_lr))
print('Recall Score Linear regression model:',recall_score(y_test,y_pred_lr))
print('F1-Score Linear regression model:',f1_score(y_test,y_pred_lr))
print()
print()


par_svm= {
    'kernel':['linear','polynomial','radial'],
    'C':[0.1,1,10],
    'gamma':['auto','scale']
}

grid_svm= GridSearchCV(svm, par_svm, n_jobs=-1, cv=5,scoring='accuracy')
grid_svm.fit(x_train,y_train)
best_svm=grid_svm.best_params_
print('Best parameters  for SVM found:', best_svm)
svm=grid_svm.best_estimator_
svm.fit(x_train,y_train)
y_pred_svm=svm.predict(x_test)
print('Accuracy Score SVM Model:',accuracy_score(y_test,y_pred_svm))
print('Precision Score SVM Model:',precision_score(y_test,y_pred_svm))
print('Recall Score SVM Model:',recall_score(y_test,y_pred_svm))
print('F1-Score SVM Model:',f1_score(y_test,y_pred_svm))
print()
print()



par_dt= {
    'max_depth':[5,10,20],
    'min_samples_split':[2,4,6],
    'max_features':[2,5,6],
    'min_samples_leaf':[2,4,6]
}

grid_dt= GridSearchCV(dt, par_dt, n_jobs=-1, cv=5,scoring='accuracy')
grid_dt.fit(x_train,y_train)
best_dt=grid_dt.best_params_
print('Best parameters  for DT found:', best_dt)
dt=grid_dt.best_estimator_
dt.fit(x_train,y_train)
y_pred_dt=dt.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_dt))
print('Precision Score:',precision_score(y_test,y_pred_dt))
print('Recall Score:',recall_score(y_test,y_pred_dt))
print('F1-Score:',f1_score(y_test,y_pred_dt))

"""# Fine tuning Neural Networks"""

par_ann= {
    'hidden_layer_sizes':[(100,),(50,50),(50,100,50)],
    'activation':['relu','tanh','logistic'],
    'solver':['lbfgs','sgd','adam'],
    'learning_rate':['adaptive','constant'],
    'alpha':[0.0001,0.001,0.1]

}

grid_ann= GridSearchCV(ann, par_ann, n_jobs=-1, cv=5,scoring='accuracy')
grid_ann.fit(x_train,y_train)
best_ann=grid_ann.best_params_
print('Best parameters  for ANN found:', best_ann)
ann=grid_ann.best_estimator_
ann.fit(x_train,y_train)
y_pred_ann=ann.predict(x_test)
print('Accuracy Score:',accuracy_score(y_test,y_pred_ann))
print('Precision Score:',precision_score(y_test,y_pred_ann))
print('Recall Score:',recall_score(y_test,y_pred_ann))
print('F1-Score:',f1_score(y_test,y_pred_ann))

# ROC Curve for Ensemble Algorithms
Q = rf.predict_proba(x_test)[:,1]
R = xg.predict_proba(x_test)[:,1]
S = lg.predict_proba(x_test)[:,1]
#ROC PLOT
fpr, tpr, _ = roc_curve(y_test, Q)
roc_auc = auc(fpr,tpr)
fpr1, tpr1, _ = roc_curve(y_test, R)
roc_auc1 = auc(fpr1,tpr1)
fpr2, tpr2, _ = roc_curve(y_test, S)
roc_auc2 = auc(fpr2,tpr2)
plt.figure(figsize=(10,7))

plt.plot(fpr, tpr, color='darkorange', label='Random Forest(AUC = %0.4f)' % roc_auc)
plt.plot(fpr1, tpr1, color='red', label='Xgboost (AUC = %0.4f)' % roc_auc1)
plt.plot(fpr2, tpr2, color='green', label='LightGbm (AUC = %0.4f)' % roc_auc2)
plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic curve For Ensemble Algorithms')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Traditional Algorithms
X = lg.predict_proba(x_test)[:,1]
Y = dt.predict_proba(x_test)[:,1]
Z = svm.predict_proba(x_test)[:,1]
#ROC PLOT
fpr, tpr, _ = roc_curve(y_test, X)
roc_auc = auc(fpr,tpr)
fpr1, tpr1, _ = roc_curve(y_test, Y)
roc_auc1 = auc(fpr1,tpr1)
fpr2, tpr2, _ = roc_curve(y_test, Z)
roc_auc2 = auc(fpr2,tpr2)
plt.figure(figsize=(10,7))

plt.plot(fpr, tpr, color='darkorange', label='Logistic Regression (AUC = %0.4f)' % roc_auc)
plt.plot(fpr1, tpr1, color='red', label='Decision Tree (AUC = %0.4f)' % roc_auc1)
plt.plot(fpr2, tpr2, color='green', label='SVM (AUC = %0.4f)' % roc_auc2)
plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic curve For Traditional Algorithms')
plt.legend(loc="lower right")
plt.show()

J= lstm.predict(x_test)[:,0]
Z = ann.predict_proba(x_test)[:,1]
R= cnn.predict(x_test)[:,0]
fpr, tpr, _ = roc_curve(y_test, J)
roc_auc = auc(fpr,tpr)
fpr1, tpr1, _ = roc_curve(y_test, R)
roc_auc1 = auc(fpr1,tpr1)
fpr2, tpr2, _ = roc_curve(y_test, Z)
roc_auc2 = auc(fpr2,tpr2)
plt.figure(figsize=(10,7))

plt.plot(fpr, tpr, color='darkorange', label='LSTM (AUC = %0.4f)' % roc_auc)
plt.plot(fpr1, tpr1, color='red', label='CNN (AUC = %0.4f)' % roc_auc1)
plt.plot(fpr2, tpr2, color='green', label='ANN (AUC = %0.4f)' % roc_auc2)
plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic curve For Neural Network Algorithms')
plt.legend(loc="lower right")
plt.show()



